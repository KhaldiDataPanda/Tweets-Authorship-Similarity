{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 1- Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T22:49:26.118728Z","iopub.status.busy":"2024-03-15T22:49:26.118112Z","iopub.status.idle":"2024-03-15T22:49:34.077598Z","shell.execute_reply":"2024-03-15T22:49:34.076770Z","shell.execute_reply.started":"2024-03-15T22:49:26.118701Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.model_selection import train_test_split\n","import random\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2- Data Preparation "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Reading the data and saving it to 1 file\n","\n","train_data = pd.read_csv(\"train.csv\")\n","test_data = pd.read_excel(\"test.xlsx\")\n","\n","combined_data = pd.concat([train_data, test_data])\n","shuffled_data = combined_data.sample(frac=1).reset_index(drop=True)\n","\n","shuffled_data.to_csv(\"train.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Defineing data preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = ''.join([c for c in text if c.isalnum() or c in [' ']])  \n","    return text\n","\n","dataset = pd.read_csv(\"/kaggle/input/simple-text-dataset-tweets/train.csv\")  # Replace \"train.csv\" with the path to your dataset\n","dataset['text'] = dataset['text'].apply(preprocess_text)\n","\n","print (\"debug 1\")"]},{"cell_type":"markdown","metadata":{},"source":["## 3- Data Processing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sample tweet pairs and assign labels\n","def create_tweet_pairs(dataset, num_pairs_per_author=800, same_author_ratio=0.30):\n","    tweet_pairs = []\n","    authors = dataset['user'].unique()\n","\n","    # Dictionary to store used tweet indices for each author\n","    used_tweet_indices = {author: set() for author in authors}\n","\n","    for author in authors:\n","        tweets_by_author = dataset[dataset['user'] == author]['text'].tolist()\n","        num_same_author_pairs = int(num_pairs_per_author * same_author_ratio)\n","        num_diff_author_pairs = num_pairs_per_author - num_same_author_pairs\n","        \n","        # Sample tweet pairs from the same author\n","        sampled_indices_same = np.random.choice(len(tweets_by_author), size=(num_same_author_pairs, 2), replace=False)\n","        for idx1, idx2 in sampled_indices_same:\n","            if (idx1 not in used_tweet_indices[author]) and (idx2 not in used_tweet_indices[author]):\n","                tweet_pairs.append((tweets_by_author[idx1], tweets_by_author[idx2], author, author, 1))\n","                used_tweet_indices[author].add(idx1)\n","                used_tweet_indices[author].add(idx2)\n","        \n","        # Sample tweet pairs from different authors\n","        other_authors = authors[authors != author]\n","        num_pairs_per_diff_author = num_diff_author_pairs // len(other_authors)\n","        remainder_pairs = num_diff_author_pairs % len(other_authors)\n","        \n","        for other_author in other_authors:\n","            tweets_by_other_author = dataset[dataset['user'] == other_author]['text'].tolist()\n","            sampled_indices_diff = np.random.choice(len(tweets_by_other_author), size=(num_pairs_per_diff_author, 2), replace=False)\n","            for idx1, idx2 in sampled_indices_diff:\n","                if (idx1 not in used_tweet_indices[author]) and (idx2 not in used_tweet_indices[other_author]):\n","                    tweet_pairs.append((tweets_by_author[np.random.randint(len(tweets_by_author))], tweets_by_other_author[idx1], author, other_author, 0))\n","                    used_tweet_indices[author].add(idx1)\n","                    used_tweet_indices[other_author].add(idx2)\n","        \n","        # Add remainder pairs\n","        tweets_by_other_author = dataset[dataset['user'] == other_authors[-1]]['text'].tolist()  # Last author\n","        sampled_indices_diff = np.random.choice(len(tweets_by_other_author), size=(remainder_pairs, 2), replace=False)\n","        for idx1, idx2 in sampled_indices_diff:\n","            if (idx1 not in used_tweet_indices[author]) and (idx2 not in used_tweet_indices[other_authors[-1]]):\n","                tweet_pairs.append((tweets_by_author[np.random.randint(len(tweets_by_author))], tweets_by_other_author[idx1], author, other_authors[-1], 0))\n","                used_tweet_indices[author].add(idx1)\n","                used_tweet_indices[other_authors[-1]].add(idx2)\n","                \n","        #shuffle the tweet pairs\n","        random.shuffle(tweet_pairs)\n","        \n","    return pd.DataFrame(tweet_pairs, columns=['tweet1', 'tweet2', 'author1', 'author2', 'target'])\n","\n","\n","\n","# Split dataset into train and test sets\n","train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42,shuffle=True)\n","\n","\n","train_pairs = create_tweet_pairs(train_data)\n","test_pairs = create_tweet_pairs(test_data)\n","\n","print (\"debug 2\")\n","print(train_pairs.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 4- Modeling & Eval"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T19:47:13.665574Z","iopub.status.busy":"2024-03-15T19:47:13.664993Z","iopub.status.idle":"2024-03-15T20:00:28.415731Z","shell.execute_reply":"2024-03-15T20:00:28.414638Z","shell.execute_reply.started":"2024-03-15T19:47:13.665547Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["debug 1\n","debug 2\n","(5273, 5)\n","debug 3\n","Index(['Unnamed: 0', 'user', 'text'], dtype='object')\n","debug 4\n","debug 5\n","Precision: 0.7656021095810138, Recall: 1.0, F1 Score: 0.8672419515433123\n"]}],"source":["class TweetSimilarityModel(nn.Module):\n","    def __init__(self, transformer_model):\n","        super(TweetSimilarityModel, self).__init__()\n","        self.transformer = transformer_model\n","        self.fc = nn.Linear(768, 1)  # Output a single similarity score\n","        self.fc2 = nn.Linear(1,1)\n","\n","    def forward(self, input_ids1, attention_mask1,input_ids2, attention_mask2):\n","        output1 = self.transformer(input_ids1, attention_mask1)[0] \n","        output2 = self.transformer(input_ids2, attention_mask2)[0]# BERT output\n","        output1 = torch.mean(output1, dim=1)  # Mean pooling over the tokens\n","        output2 = torch.mean(output2, dim=1) \n","        output1 = self.fc(output1)\n","        output2 = self.fc(output2)\n","        output1 = torch.sigmoid(output1)\n","        output2 = torch.sigmoid(output2)\n","        distance = torch.abs(output1 - output2)\n","        ret = self.fc2(distance)\n","        \n","        return  torch.sigmoid(ret)\n","\n","# Load pre-trained transformer model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = AutoModel.from_pretrained(\"bert-base-uncased\")\n","print(\"debug 3\")\n","\n","\n","\n","# Define model, optimizer, and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TweetSimilarityModel(model).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","criterion = nn.BCELoss()\n","\n","\n","\n","\n","\n","\n","\n","# Training loop\n","\n","\n","num_epochs = 2\n","for epoch in range(num_epochs):\n","    model.train()\n","    for index, row in train_pairs.iterrows():\n","        input_ids1 = tokenizer(row['tweet1'], return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n","        input_ids2 = tokenizer(row['tweet2'], return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n","        attention_mask1 = tokenizer(row['tweet1'], return_tensors='pt', padding=True, truncation=True)['attention_mask'].to(device)\n","        attention_mask2 = tokenizer(row['tweet2'], return_tensors='pt', padding=True, truncation=True)['attention_mask'].to(device)\n","        label = torch.tensor(row['target'], dtype=torch.float).to(device)\n","      \n","        optimizer.zero_grad()\n","        similarity_score = model( input_ids1, attention_mask1,input_ids2, attention_mask2)\n","        \n","        \n","        loss = criterion(similarity_score.squeeze(), label)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","print(\"debug 4\")\n","\n","\n","\n","\n","\n","\n","# Evaluation\n","\n","\n","model.eval()\n","#threshold=0.515\n","predicted_labels = []\n","true_labels = []\n","with torch.no_grad():\n","    for index, row in test_pairs.iterrows():\n","        input_ids1 = tokenizer(row['tweet1'], return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n","        input_ids2 = tokenizer(row['tweet2'], return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n","        attention_mask1 = tokenizer(row['tweet1'], return_tensors='pt', padding=True, truncation=True)['attention_mask'].to(device)\n","        attention_mask2 = tokenizer(row['tweet2'], return_tensors='pt', padding=True, truncation=True)['attention_mask'].to(device)\n","        label = torch.tensor(row['target'], dtype=torch.float).to(device)\n","\n","        similarity_score = model( input_ids1, attention_mask1,input_ids2, attention_mask2)\n","        \n","        predicted_label = torch.round(similarity_score.squeeze()).cpu().numpy()\n","       \n","        \n","    \n","        predicted_labels.append(predicted_label)\n","        true_labels.append(row['target'])\n","        \n","        \n","print(\"debug 5\")\n","\n","\n","predicted_labels = np.hstack(predicted_labels)\n","true_labels = np.array(true_labels)\n","\n","precision = precision_score(true_labels, predicted_labels)\n","recall = recall_score(true_labels, predicted_labels)\n","f1 = f1_score(true_labels, predicted_labels)\n","\n","print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n","\n","## important : the real scores are : Precision: 0.7819526627218935, Recall: 1.0, F1 Score: 0.8776357297028059\n","## check out the kaggle notebook for the real scores "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4600355,"sourceId":7845853,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
